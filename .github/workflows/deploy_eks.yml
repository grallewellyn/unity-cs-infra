name: EKS Deployment
env:
  EKSClusterRegion: us-west-2
  EKSKubeProxyVersion: latest
  EKSCoreDNSVersion: latest
  EKSEBSCSIVersion: latest
# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  # push:
  #   branches: [ ucs-template ]
  # pull_request:
  #   branches: [ ucs-template ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    branches: [ main ]
    inputs:
      distinct_id:
      KEY:
        description: 'Access Key ID'
        required: false
        type: string
        default: ''
      SECRET:
        description: 'Access Secret Key ID'
        required: false
        type: string
        default: ''
      TOKEN:
        description: 'AWS Session Token'
        required: false
        type: string
        default: ''
      META:
        description: 'metadata description'
        required: true
        type: string
      AWSCONNECTION:
        description: 'Method of AWS connection'
        required: true
        type: choice
        default: 'oidc'
        options:
        - oidc
        - keys
        - iam
      DEPLOYMENTPROJECT:
        description: 'Deployment Project'
        required: true 
        type: choice
        default: 'UNITY'
        options: 
        - UNITY
        - SIPS
      DEPLOYMENTSTAGE:
        description: 'Deployment Target'
        required: true 
        type: choice
        default: 'DEV'
        options: 
        - DEV
        - TEST
        - OPS
        - SIPS
permissions:
  id-token: write # required to use OIDC authentication
  contents: read # required to checkout the code from the repo

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  deployment:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: echo distinct ID ${{ github.event.inputs.distinct_id }}
        run: echo ${{ github.event.inputs.distinct_id }}
      # Set up current working directory with the repo contents
      - uses: actions/checkout@v3
      - name: Install aws
        if: ${{ github.event.act }}
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

      - name: Install kubectl
        if: ${{ github.event.act }}
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl      
      - name: Set env vars
        run: |
          if [ "${{ INPUTS.DEPLOYMENTSTAGE }}" == "DEV" ];
          then
            echo "EKSClusterVersion=${{ vars.MCP_DEV_EKSCLUSTERVERSION }}" >> $GITHUB_ENV
            echo "EKSClusterAMI=${{ vars.MCP_DEV_EKSCLUSTERAMI }}" >> $GITHUB_ENV
            echo "EKSSecurityGroup=${{ vars.MCP_DEV_EKSSECURITYGROUP }}" >> $GITHUB_ENV
            echo "EKSSharedNodeSecurityGroup=${{ vars.MCP_DEV_EKSSHAREDNODESECURITYGROUP }}" >> $GITHUB_ENV
            echo "EKSSubnetConfigA=${{ vars.MCP_DEV_EKSSUBNETCONFIGA }}" >> $GITHUB_ENV
            echo "EKSSubnetConfigB=${{ vars.MCP_DEV_EKSSUBNETCONFIGB }}" >> $GITHUB_ENV
            echo "EKSInstanceRoleArn=${{ vars.MCP_DEV_EKSINSTANCEROLEARN }}" >> $GITHUB_ENV
            echo "EKSServiceArn=${{ vars.MCP_DEV_EKSSERVICEARN }}" >> $GITHUB_ENV
          elif [ "${{ INPUTS.DEPLOYMENTSTAGE }}" == "TEST" ];
          then
            echo "EKSClusterVersion=${{ vars.MCP_TEST_EKSCLUSTERVERSION }}" >> $GITHUB_ENV
            echo "EKSClusterAMI=${{ vars.MCP_TEST_EKSCLUSTERAMI }}" >> $GITHUB_ENV
            echo "EKSSecurityGroup=${{ vars.MCP_TEST_EKSSECURITYGROUP }}" >> $GITHUB_ENV
            echo "EKSSharedNodeSecurityGroup=${{ vars.MCP_TEST_EKSSHAREDNODESECURITYGROUP }}" >> $GITHUB_ENV
            echo "EKSSubnetConfigA=${{ vars.MCP_TEST_EKSSUBNETCONFIGA }}" >> $GITHUB_ENV
            echo "EKSSubnetConfigB=${{ vars.MCP_TEST_EKSSUBNETCONFIGB }}" >> $GITHUB_ENV
            echo "EKSInstanceRoleArn=${{ vars.MCP_TEST_EKSINSTANCEROLEARN }}" >> $GITHUB_ENV
            echo "EKSServiceArn=${{ vars.MCP_TEST_EKSSERVICEARN }}" >> $GITHUB_ENV
          fi
      # Configure AWS Credentials through OIDC
      - name: Configure AWS credentials
        if: ${{ INPUTS.AWSCONNECTION == 'oidc' }}
        uses: aws-actions/configure-aws-credentials@v1
        with:
          #role-to-assume: ${{ secrets.OIDC_ROLE }}
          role-to-assume: ${{ secrets[format('OIDC_{0}_ROLE', INPUTS.DEPLOYMENTSTAGE)] }}
          aws-region: ${{ vars.AWS_REGION }}

      # Install eksctl to launch EKS
      - name: Install eksctl
        run: |
         curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp && \
         sudo mv /tmp/eksctl /usr/local/bin && \
         eksctl version

      - name: Install Unity Transformer
        run: |
          curl --silent --location https://github.com/unity-sds/unity-cs-manager/releases/download/0.1.23-Alpha/unity-cs-manager-0.1.23-Alpha-linux-amd64.tar.gz | tar xz -C /tmp
          sudo mv /tmp/unity-cs-manager /usr/local/bin

      # Render template
      - name: Render Template
        run: |
          export owner=$(echo '${{ inputs.META }}' | jq -r .owner)
          export cluster=$(echo '${{ inputs.META }}' | jq -r .clustername)
          export minnodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export maxnodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export desirednodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export instancetype=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.instancetype)
          unity-cs-manager eks --name ${cluster} --owner ${owner} --managenodegroups defaultgroup,${minnodes},${maxnodes},${desirednodes},m5.xlarge --instancetype ${instancetype} --projectname ${owner} --servicename ${owner}  --capability unset --capversion unset --component unset --creator unset --critinfra unset --experimental unset --exposed unset --pocs test --release unset --securityplan unset --sourcecontrol unset --userfacing unset --venue unset --servicearea unset --resourcename=${cluster} --applicationname unset --applicationversion unset > /tmp/eksctl-config.yaml
          cat /tmp/eksctl-config.yaml
      # Launch EKS
      - name: Launch EKS cluster
        run: |
          if [ "${{inputs.AWSCONNECTION}}" == "keys" ]
          then
              export AWS_ACCESS_KEY_ID=${{ inputs.KEY }}
              export AWS_SECRET_ACCESS_KEY=${{ inputs.SECRET }}
              export AWS_SESSION_TOKEN=${{ inputs.TOKEN }}
              export AWS_PAGER=""
          fi
          export cluster=$(echo '${{ inputs.META }}' | jq -r .clustername)
          eksctl create cluster -f /tmp/eksctl-config.yaml
          aws eks update-kubeconfig --region us-west-2 --name $cluster
          kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
          eksctl create iamidentitymapping --cluster ${cluster} --region=us-west-2 --arn arn:aws:iam::237868187491:role/mcp-tenantDeveloper --group system:masters --username admin
          eksctl create iamidentitymapping --cluster ${cluster} --region=us-west-2 --arn arn:aws:iam::237868187491:role/mcp-tenantOperator --group system:masters --username adminOp

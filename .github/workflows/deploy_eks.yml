name: EKS Deployment
env:
  EKSClusterRegion: us-west-2
  EKSKubeProxyVersion: latest
  EKSCoreDNSVersion: latest
  EKSEBSCSIVersion: latest
# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  # push:
  #   branches: [ ucs-template ]
  # pull_request:
  #   branches: [ ucs-template ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    branches: [ main ]
    inputs:
      distinct_id:
      TEARDOWN:
        description: 'Teardown EKS Cluster?'
        required: false
        type: boolean
        default: false
      KEY:
        description: 'Access Key ID'
        required: false
        type: string
        default: ''
      SECRET:
        description: 'Access Secret Key ID'
        required: false
        type: string
        default: ''
      TOKEN:
        description: 'AWS Session Token'
        required: false
        type: string
        default: ''
      META:
        description: 'metadata description'
        required: true
        type: string
      AWSCONNECTION:
        description: 'Method of AWS connection'
        required: true
        type: choice
        default: 'oidc'
        options:
        - oidc
        - keys
        - iam
      DEPLOYMENTPROJECT:
        description: 'Deployment Project'
        required: true 
        type: choice
        default: 'UNITY'
        options: 
        - UNITY
        - SIPS
      DEPLOYMENTSTAGE:
        description: 'Deployment Target'
        required: true 
        type: choice
        default: 'DEV'
        options: 
        - DEV
        - TEST
        - OPS
        - SIPS
permissions:
  id-token: write # required to use OIDC authentication
  contents: read # required to checkout the code from the repo

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  deployment:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - name: echo distinct ID ${{ github.event.inputs.distinct_id }}
        run: echo ${{ github.event.inputs.distinct_id }}
      # Set up current working directory with the repo contents
      - uses: actions/checkout@v3
      - name: Install aws
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl      
      - name: Set env vars
        run: | 
            echo "EKSClusterVersion=1.24" >> $GITHUB_ENV
            echo "EKSClusterAMI=ami-0886544fa915698f0" >> $GITHUB_ENV
            echo "EKSSecurityGroup=sg-09bd8de0af1c3c99a" >> $GITHUB_ENV
            echo "EKSSharedNodeSecurityGroup=sg-09bd8de0af1c3c99a" >> $GITHUB_ENV
            echo "EKSPublicSubnetA=us-west-2c: {id: subnet-087b54673c7549e2d}" >> $GITHUB_ENV
            echo "EKSPublicSubnetB=us-west-2d: {id: subnet-009c32904a8bf3b92}" >> $GITHUB_ENV
            echo "EKSPrivateSubnetA=us-west-2c: {id: subnet-0ba7d45fe29659a2a}" >> $GITHUB_ENV
            echo "EKSPrivateSubnetB=us-west-2d: {id: subnet-0e4ff7f670ebb4cc3}" >> $GITHUB_ENV
            echo "EKSInstanceRoleArn=arn:aws:iam::237868187491:role/Unity-UCS-Development-EKSNodeRole" >> $GITHUB_ENV
            echo "EKSServiceArn=arn:aws:iam::237868187491:role/Unity-UCS-Development-EKSClusterS3-Role" >> $GITHUB_ENV
            echo "EKSUserArn=arn:aws:iam::237868187491:role/mcp-tenantDeveloper;arn:aws:iam::237868187491:role/mcp-tenantOperator" >> $GITHUB_ENV
      # Configure AWS Credentials through OIDC
      - name: Configure AWS credentials
        if: ${{ INPUTS.AWSCONNECTION == 'oidc' }}
        uses: aws-actions/configure-aws-credentials@v1
        with:
          #role-to-assume: ${{ secrets.OIDC_ROLE }}
          role-to-assume: ${{ secrets[format('OIDC_{0}_ROLE', INPUTS.DEPLOYMENTSTAGE)] }}
          aws-region: ${{ vars.AWS_REGION }}

      # Install eksctl to launch EKS
      - name: Install eksctl
        run: |
         curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/v0.132.0/eksctl_Linux_amd64.tar.gz" | tar xz -C /tmp && \
         sudo mv /tmp/eksctl /usr/local/bin && \
         eksctl version

      - name: Install Unity Transformer
        run: |
          curl --silent --location https://github.com/unity-sds/unity-cs-manager/releases/download/0.1.26-Alpha/unity-cs-manager-0.1.26-Alpha-linux-amd64.tar.gz | tar xz -C /tmp
          sudo mv /tmp/unity-cs-manager /usr/local/bin

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
          chmod 700 get_helm.sh
          ./get_helm.sh

      # Render template
      - name: Render Template
        run: |
          export owner=$(echo '${{ inputs.META }}' | jq -r .owner)
          export cluster=$(echo '${{ inputs.META }}' | jq -r .clustername)
          export minnodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export maxnodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export desirednodes=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.nodecount)
          export instancetype=$(echo '${{ inputs.META }}' | jq -r .nodegroups.group1.instancetype)
          export private=$(echo '${{ inputs.META }}' | jq -r .private)
          if [ $private == "true" ]
          then
            unset EKSPublicSubnetA
            unset EKSPublicSubnetB
          fi
          unity-cs-manager eks --name ${cluster} --owner ${owner} --managenodegroups defaultgroup,${minnodes},${maxnodes},${desirednodes},m5.xlarge --instancetype ${instancetype} --projectname ${owner} --servicename ${owner}  --capability unset --capversion unset --component unset --creator unset --critinfra unset --experimental unset --exposed unset --pocs test --release unset --securityplan unset --sourcecontrol unset --userfacing unset --venue unset --servicearea unset --resourcename=${cluster} --applicationname unset --applicationversion unset > /tmp/eksctl-config.yaml
          cat /tmp/eksctl-config.yaml

      # Launch EKS
      - name: Launch EKS cluster
        if: "${{ ! inputs.TEARDOWN }}"
        run: |
          if [ "${{inputs.AWSCONNECTION}}" == "keys" ]
          then
              export AWS_ACCESS_KEY_ID=${{ inputs.KEY }}
              export AWS_SECRET_ACCESS_KEY=${{ inputs.SECRET }}
              export AWS_SESSION_TOKEN=${{ inputs.TOKEN }}
              export AWS_PAGER=""
          fi
          export cluster=$(echo '${{ inputs.META }}' | jq -r .clustername)
          eksctl create cluster -f /tmp/eksctl-config.yaml
          aws eks update-kubeconfig --region us-west-2 --name $cluster
          kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
          export IFS=";"
          sentence="$EKSUserArn"
          for word in $sentence; do
            echo "$word"
            #eksctl create iamidentitymapping --cluster ${cluster} --region=us-west-2 --arn arn:aws:iam::237868187491:role/mcp-tenantDeveloper --group system:masters --username admin
            #eksctl create iamidentitymapping --cluster ${cluster} --region=us-west-2 --arn arn:aws:iam::237868187491:role/mcp-tenantOperator --group system:masters --username adminOp
            eksctl create iamidentitymapping --cluster ${cluster} --region=us-west-2 --arn $word --group system:masters --username admin
          done
          helm repo add fairwinds-stable https://charts.fairwinds.com/stable
          helm install vpa fairwinds-stable/vpa --namespace vpa --create-namespace
          helm install goldilocks --namespace goldilocks --create-namespace fairwinds-stable/goldilocks

      # Teardown EKS
      - name: Teardown EKS cluster
        if: ${{ inputs.TEARDOWN }}
        run: |
          if [ "${{inputs.AWSCONNECTION}}" == "keys" ]
          then
              export AWS_ACCESS_KEY_ID=${{ inputs.KEY }}
              export AWS_SECRET_ACCESS_KEY=${{ inputs.SECRET }}
              export AWS_SESSION_TOKEN=${{ inputs.TOKEN }}
              export AWS_PAGER=""
          fi
          export IFS=";"
          export cluster=$(echo '${{ inputs.META }}' | jq -r .clustername)
          aws eks update-kubeconfig --region us-west-2 --name $cluster
          helm repo add fairwinds-stable https://charts.fairwinds.com/stable
          helm uninstall vpa --namespace vpa
          helm uninstall goldilocks --namespace goldilocks
          eksctl delete nodegroup defaultgroupNodeGroup --cluster $cluster --drain=false --disable-eviction
          eksctl delete cluster --name $cluster
